{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc89e04",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Sarcasm has been part of our language for many years. It is the caustic use of irony, in which words are used to communicate the opposite of their surface meaning, in a humorous way or to mock someone or something. Understanding sarcasm is not always obvious, because it depends on your language skills and your knowledge of other people’s minds. For example, how would you classify the sentence “What a fantastic musician!”? Detecting sarcasm is much harder in text, as there are no additional cues. But what about a computer? Is it possible to train a machine learning model that can detect whether a sentence is sarcastic or not? \n",
    "\n",
    "\n",
    "## Problem statement\n",
    "The goal of this project is to predict whether a comment is sarcastic or not based on 1 million comments scrapped from Reddit - also called sub-reddits. This means, we are facing a binary classification problem that involves incorporating NLP techniques to feed to our ML/DL models.\n",
    "\n",
    "\n",
    "## Goal of this notebook\n",
    "Before we head straight to the modelling part, we need to analyse our data in order to understand:\n",
    "- Is our dataset balanced? \n",
    "- Is part of our data skewing the results?\n",
    "- Can we detect strange data distributions beforehand? \n",
    "- Do we need to clean the data before modelling?\n",
    "\n",
    "## Structure of this notebook\n",
    "- Set-up\n",
    "- Data inspection \n",
    "- Exploratory Data Analysis\n",
    "- Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4ecd9",
   "metadata": {},
   "source": [
    "# 0. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c3ddc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Import all the necessary packages and custom functions (from the functions.py file)\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from datetime import datetime\n",
    "from plotly import *\n",
    "import plotly.graph_objs as go\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords') <-- You may need to download it if you encounter a LookupError \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec5bd578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Get the data from the Google Drive public folders\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Load train and test dataframes\n",
    "test_df = get_sarcasm_test_df()\n",
    "train_df = get_sarcasm_train_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b85eb",
   "metadata": {},
   "source": [
    "# 1. Data cleansing\n",
    "Why is this important? Predictive models, regardless of the sophistication of the algorithms employed, are only as good as the data used to train them. Incorrect data yields inaccurate insights. In addition, poorly formatted data can’t easily be sorted by computers. \n",
    "\n",
    "Since we will create 2 different models, the data cleaning part may be a bit different. While RoBERTa model handles (and actually needs) stopwords in order to figure out the context of the sentence, Logistic Regression will benefit from cleaning stopwords and punctuations to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e73df",
   "metadata": {},
   "source": [
    "## 1.1 Inspecting the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f27b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Identifying missing value in the datasets\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Do we have any duplicate rows in the datasets?\n",
    "print('Duplicate rows in the train dataset:',len(train_df[train_df.duplicated() == True]))\n",
    "print('Duplicate rows in the test dataset:',len(test_df[test_df.duplicated() == True]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the training dataset\n",
    "print(\"The 'comment' column is the main field we want to use to classify whether the comment is sarcastic or not\")\n",
    "print(\"The column 'label' defines if a comment is sarcastic (sarcastic = 1) or not (genuine = 0)\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48568845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have any missing value in the dataset?\n",
    "train_df.info()\n",
    "print(\"---------------------------------\")\n",
    "print(\"It seems that we are missing 47 comments in the 'comments' column. We will filter them out\")\n",
    "train_df = train_df[train_df.comment.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fd888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the test dataset\n",
    "print(\"The structure of the dataset is identical to the training dataset, except here we are missing the 'label' column\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd910ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have any missing value in the dataset?\n",
    "test_df.info()\n",
    "print(\"---------------------------------\")\n",
    "print(\"The test dataframe is missing some comments too. Since this field is critical, we will also filter those comments out.\")\n",
    "test_df = test_df[test_df.comment.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92bc1f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#####\n",
    "##### Dataset balance: Are sarcastic and non-sarcastic comments equally distributed? \n",
    "#####\n",
    "###################################################################################################\n",
    "\n",
    "# Every comment is unique, so we want to check the proportion of sarcastic and non-sacartic comments\n",
    "\n",
    "\n",
    "train_df[\"label\"].value_counts().plot(kind = 'pie', autopct='%1.1f%%', figsize=(6, 6)).legend()\n",
    "print(round(train_df.id[train_df.label == 1].count() / train_df.id.count()*100,2), \"% - perfectly balanced! An amazing dataset! This points us to consider accuracy as one of our key performance metrics in the modelling part, as it is easy to interpret. However we also want to check the model performance based on another metric like precision or recall.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6271a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#####\n",
    "##### Is score an indicator of sarcasm?\n",
    "#####\n",
    "###################################################################################################\n",
    "\n",
    "# Create dataframes for sarcastic and genuine comments\n",
    "score_sarcastic = train_df[train_df['label']==1]\n",
    "score_genuine = train_df[train_df['label']==0]\n",
    "\n",
    "# Apply function from the functions.py file to classify comments in groups by their score \n",
    "score_sarcastic['score_groups'] = score_sarcastic.apply(score_classification, axis = 1)\n",
    "score_genuine['score_groups'] = score_genuine.apply(score_classification, axis = 1)\n",
    "\n",
    "# Get the percentage over the total for each group\n",
    "score_sarcastic_gb = score_sarcastic.groupby(['score_groups'])['id'].nunique().reset_index()\n",
    "score_sarcastic_gb['total'] = score_sarcastic_gb.id.sum()\n",
    "score_sarcastic_gb['percentage_over_total_sarcastic'] = round(score_sarcastic_gb.id / score_sarcastic_gb.total * 100,2)\n",
    "score_sarcastic_gb = score_sarcastic_gb[['score_groups', 'percentage_over_total_sarcastic']]\n",
    "\n",
    "# Get the percentage over the total for each group\n",
    "score_genuine_gb = score_genuine.groupby(['score_groups'])['id'].nunique().reset_index()\n",
    "score_genuine_gb['total'] = score_genuine_gb.id.sum()\n",
    "score_genuine_gb['percentage_over_total_genuine'] = round(score_genuine_gb.id / score_genuine_gb.total * 100,2)\n",
    "score_genuine_gb = score_genuine_gb[['score_groups', 'percentage_over_total_genuine']]\n",
    "\n",
    "print(\"There are minor differences between the scores on sarcastic and non-sarcastic comments based on the score they get. Therefore, score may not be a relevant attribute for the feature generation part\")\n",
    "\n",
    "score_sarcastic_gb.merge(score_genuine_gb, how='left', on='score_groups')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#####\n",
    "##### Are certain authors skewing our data?\n",
    "#####\n",
    "###################################################################################################\n",
    "\n",
    "# Count the number of sarcastic comments per author\n",
    "sarcastic_authors = train_df[train_df['label']==1].groupby(['author'])['id'].nunique().reset_index()\n",
    "sarcastic_authors.columns = ['author', 'sarcastic_comments']\n",
    "\n",
    "# Count the number of non-sarcastic comments per author\n",
    "non_sarcastic_authors = train_df[train_df['label']==0].groupby(['author'])['id'].nunique().reset_index()\n",
    "non_sarcastic_authors.columns = ['author', 'non_sarcastic_comments']\n",
    "\n",
    "# Create total dataset\n",
    "authors_df = sarcastic_authors.merge(non_sarcastic_authors, how='left', on ='author')\n",
    "\n",
    "# Data transformations where needed\n",
    "authors_df.non_sarcastic_comments = authors_df.non_sarcastic_comments.fillna(0)\n",
    "authors_df.sarcastic_comments = authors_df.sarcastic_comments.fillna(0)\n",
    "authors_df.non_sarcastic_comments = authors_df.non_sarcastic_comments.astype(int)\n",
    "\n",
    "# Create column with total comments per author (sarcastic and non-sarcastic)\n",
    "authors_df['total_comments'] = authors_df.sarcastic_comments + authors_df.non_sarcastic_comments\n",
    "\n",
    "# Create column with 'sarcastic rate' per author - sarcastic comments / total comments\n",
    "authors_df['sarcastic_rate'] = authors_df.sarcastic_comments / authors_df.total_comments\n",
    "\n",
    "# Is our dataset balanced at a user level too?\n",
    "print(\"We find out that\", round(authors_df[authors_df.sarcastic_rate == 0.50].author.nunique() / authors_df.author.nunique()*100,2), \"% of the users post as many geniune as sarcastic comments\")\n",
    "print(\"Only\", round(authors_df[authors_df.sarcastic_rate >= 0.70].author.nunique() / authors_df.author.nunique()*100,2), \"% of the users post sarcastic comments on 70% of the time\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd66aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#####\n",
    "##### Is data changing over time? \n",
    "#####\n",
    "###################################################################################################\n",
    "\n",
    "# Define dataframe for sarcastic and non-sarcastic comments\n",
    "sarcastic = train_df_clean[train_df_clean['label']==1]\n",
    "genuine = train_df_clean[train_df_clean['label']==0]\n",
    "\n",
    "\n",
    "# Group the data by dates for sarcastic comments\n",
    "sarcastic=sarcastic.groupby(['date'])['label'].count()\n",
    "sarcastic=pd.DataFrame(sarcastic)\n",
    "\n",
    "# Group the data by dates for non-sarcastic comments\n",
    "genuine=genuine.groupby(['date'])['label'].count()\n",
    "genuine=pd.DataFrame(genuine)\n",
    "\n",
    "# Plotting the time series graph\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "         x=genuine.index,\n",
    "         y=genuine['label'],\n",
    "         name='Genuine',\n",
    "    line=dict(color='blue'),\n",
    "    opacity=0.8))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "         x=sarcastic.index,\n",
    "         y=sarcastic['label'],\n",
    "         name='Sarcastic',\n",
    "    line=dict(color='red'),\n",
    "    opacity=0.8))\n",
    "\n",
    "fig.update_xaxes(\n",
    "    rangeslider_visible=True,\n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "            dict(step=\"all\")\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "        \n",
    "    \n",
    "fig.update_layout(title_text='Sarcastic and non-sarcastic comments',plot_bgcolor='rgb(248, 248, 255)',yaxis_title='Value')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e9f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the graph above, we may wonder... should we filter out the data from 2016 onwards? \n",
    "#    Because there seem to be more geniune than sarcastic comments. Let's inspect the distribution!\n",
    "\n",
    "# Plot distribution of data without 2016 onwards data\n",
    "train_df[train_df['date']<'2016-01'][\"label\"].value_counts().plot(kind = 'pie', autopct='%1.1f%%', figsize=(6, 6)).legend()\n",
    "\n",
    "print(\"If we remove 2016 onwards, then the perfectly balanced distribution (50%) doesn't hold anymore. Since the goal of this project is to model over text, not considering other business questions, let's keep all the data as it was initially intended (including 2016)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#####\n",
    "##### Are longer or shorter comments more sarcastic?\n",
    "#####\n",
    "###################################################################################################\n",
    "\n",
    "# Create a new column and add count of words per comment\n",
    "train_df['number_of_words'] = train_df.comment.str.split().str.len()\n",
    "\n",
    "# Apply clasisfication groups from functions.py\n",
    "train_df['number_of_words_group'] = train_df.apply(number_of_words_groups, axis = 1)\n",
    "test_df['number_of_words_group'] = train_df.apply(number_of_words_groups, axis = 1)\n",
    "\n",
    "print(\"We do not observe any indications showing that we could predict the label of a comment based on its length. Every group of comments based on comment's lenght is quite balanced\")\n",
    "\n",
    "# Aggregate data and visualize it\n",
    "pd.DataFrame(train_df.groupby(['number_of_words_group','label'])['id'].count()).reset_index().sort_values(by = 'number_of_words_group')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#####\n",
    "##### Are comment lenght distributions different between train and test datasets?\n",
    "#####\n",
    "###################################################################################################\n",
    "\n",
    "# Group the data for the training dataset\n",
    "train_df_gb = pd.DataFrame(train_df.groupby(['number_of_words_group'])['id'].count()).reset_index().sort_values(by = 'number_of_words_group')\n",
    "train_df_gb['percentage_over_total_train'] = round(train_df_gb.id / train_df_gb.id.sum()*100,2)\n",
    "\n",
    "# Group the data for the test dataset\n",
    "test_df_gb = pd.DataFrame(test_df.groupby(['number_of_words_group'])['id'].count()).reset_index().sort_values(by = 'number_of_words_group')\n",
    "test_df_gb['percentage_over_total_test'] = round(test_df_gb.id / test_df_gb.id.sum()*100,2)\n",
    "\n",
    "# Combine both datasets and compare the distributions\n",
    "df_words_combined = train_df_gb.merge(test_df_gb, on='number_of_words_group', how='left')\n",
    "\n",
    "print(\"Comment's lenght distributions are almost identical for the training and test datasets. We can be confident that the datasets are well prepared for the next step: modelling\")\n",
    "df_words_combined[['number_of_words_group', 'percentage_over_total_train', 'percentage_over_total_test']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55aae4",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "What did we learn from this notebook?\n",
    "1. The training and test datasets are very well balanced, there is no clear pattern in the data to generate features\n",
    "2. There is room for data cleansing in the datasets, eliminating punctuations, symbols, stopwords... but we need to consider what model we want to use beforehand. BERT-based models may need stopwords in order to provide better context\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
