{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Logistic Regression\n",
    "\n",
    "\n",
    "## Problem statement¶\n",
    "The goal of this project is to predict whether a comment is sarcastic or not based on 1 million comments scrapped from Reddit - also called sub-reddits. This means, we are facing a binary classification problem that involves incorporating NLP techniques to feed to our ML/DL models.\n",
    "\n",
    "## Goal of this notebook\n",
    "Our goal is to build a model to predict the comment's label (sarcastic or not). In this notebook, we will use logistic regression and TfidfVectorizer - that transforms text to feature vectors that can be used as input to estimator - in order to perform this task. Why using logistic regression? It is fast and relatively uncomplicated, and it’s convenient for you to interpret the results.\n",
    "\n",
    "## Structure of this notebook\n",
    "0. Set-up and data cleansing\n",
    "1. Modelling (4 cases using combinations of comment and parent + with/without cleaning data)\n",
    "- Create datasets and clean data when necessary\n",
    "- Modelling\n",
    "- Evaluation\n",
    "2. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==0.24.2 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from -r requirements_logit.txt (line 1)) (0.24.2)\n",
      "Requirement already satisfied: matplotlib==3.4.3 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from -r requirements_logit.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: numpy==1.20.3 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from -r requirements_logit.txt (line 3)) (1.20.3)\n",
      "Requirement already satisfied: pandas==1.3.4 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from -r requirements_logit.txt (line 4)) (1.3.4)\n",
      "Requirement already satisfied: seaborn==0.11.2 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from -r requirements_logit.txt (line 5)) (0.11.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2->-r requirements_logit.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2->-r requirements_logit.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2->-r requirements_logit.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements_logit.txt (line 2)) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements_logit.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements_logit.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements_logit.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements_logit.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.4->-r requirements_logit.txt (line 4)) (2021.3)\n",
      "Requirement already satisfied: six in /Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib==3.4.3->-r requirements_logit.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Install all the necessary packages \n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "!pip install -r requirements_logit.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Import all the necessary packages and custom functions (from the functions.py file)\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/hr/t51dnsw50m587kcwglw8j0040000gq/T/ipykernel_10526/2121889709.py\", line 9, in <module>\n",
      "    train_df = get_sarcasm_train_df()\n",
      "  File \"/Users/Alvaro_Ager/Development/reddit_comments_sarcasm_detection/notebooks/functions.py\", line 35, in get_sarcasm_train_df\n",
      "    df_1 = pd.read_csv(path, index_col=None)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n",
      "    self.handles = get_handle(\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 609, in get_handle\n",
      "    ioargs = _get_filepath_or_buffer(\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 317, in _get_filepath_or_buffer\n",
      "    reader = BytesIO(req.read())\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/http/client.py\", line 475, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/http/client.py\", line 625, in _safe_read\n",
      "    chunk = self.fp.read(min(amt, MAXAMOUNT))\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/posixpath.py\", line 392, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/posixpath.py\", line 425, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/hr/t51dnsw50m587kcwglw8j0040000gq/T/ipykernel_10526/2121889709.py\", line 9, in <module>\n",
      "    train_df = get_sarcasm_train_df()\n",
      "  File \"/Users/Alvaro_Ager/Development/reddit_comments_sarcasm_detection/notebooks/functions.py\", line 35, in get_sarcasm_train_df\n",
      "    df_1 = pd.read_csv(path, index_col=None)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 482, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 811, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1040, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 51, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\", line 222, in _open_handles\n",
      "    self.handles = get_handle(\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 609, in get_handle\n",
      "    ioargs = _get_filepath_or_buffer(\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 317, in _get_filepath_or_buffer\n",
      "    reader = BytesIO(req.read())\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/http/client.py\", line 475, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/http/client.py\", line 625, in _safe_read\n",
      "    chunk = self.fp.read(min(amt, MAXAMOUNT))\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/inspect.py\", line 1503, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/Alvaro_Ager/opt/anaconda3/lib/python3.9/inspect.py\", line 748, in getmodule\n",
      "    if f == _filesbymodname.get(modname, None):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/var/folders/hr/t51dnsw50m587kcwglw8j0040000gq/T/ipykernel_10526/2121889709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sarcasm_test_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sarcasm_train_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Development/reddit_comments_sarcasm_detection/notebooks/functions.py\u001b[0m in \u001b[0;36mget_sarcasm_train_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://drive.google.com/uc?export=download&id='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdf_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"method\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         return IOArgs(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[0;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[1;32m   3363\u001b[0m                         \u001b[0masy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3364\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0masync_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3365\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TypeError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple)\u001b[0m\n\u001b[1;32m   3170\u001b[0m                     \u001b[0minteractivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'async'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3172\u001b[0;31m                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n\u001b[0m\u001b[1;32m   3173\u001b[0m                        interactivity=interactivity, compiler=compiler, result=result)\n\u001b[1;32m   3174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[0;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[1;32m   3381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_before_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2064\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mchained_exc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n\u001b[0m\u001b[1;32m   1143\u001b[0m                                                                      chained_exceptions_tb_offset)\n\u001b[1;32m   1144\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Get the data from the Google Drive public folders\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Load train and test dataframes\n",
    "test_df = get_sarcasm_test_df()\n",
    "train_df = get_sarcasm_train_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Logistic Regression on 'comment' field without data cleaning\n",
    "\n",
    "In this case, we will use 'comment' as the text input field for the model. Moreover, this section describes the process in a detailed way, whilst the following modelling cases using logistic regression are condensed in order not to repeat the descriptions and save time and attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Prepare the datasets for the model\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Initiate\n",
    "start_time = time.time()\n",
    "\n",
    "# Keep only the necessary columns\n",
    "test_df_model = test_df[['id', 'label', 'comment']].dropna()\n",
    "train_df_model = train_df[['id', 'label', 'comment']].dropna()\n",
    "\n",
    "# Set training dataset\n",
    "X_train = train_df_model['comment']\n",
    "Y_train = train_df_model['label']\n",
    "\n",
    "# Set test dataset\n",
    "X_test = test_df_model['comment']\n",
    "Y_test = test_df_model['label']\n",
    "\n",
    "# Show how does the dataframe look like\n",
    "train_df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF(Term Frequency — Inverse Document Frequency)\n",
    "\n",
    "Term Frequency Inverse Document Frequency (TFIDF) analysis is one of the simple and robust methods to understand the context of a text. Term Frequency and Inverse Document Frequency is used to find the related content and important words and phrases in a larger text. Computers cannot understand the meaning of a text, but they can understand numbers. The words can be converted to numbers so that the relationship between them can be understood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Set up our text vectorisation and our model\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Transforming our text fields by quantifying the relevance of string representations. We will\n",
    "#  set up the vectorizer considering unigrams (1,1) and bigrams (2,2) and ignoring terms that \n",
    "#  have a document frequency strictly lower than 2\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is an efective algorithm to classify text, widely use in NLP tasks. Moreover, it is quite fast performance-wise, it is very solid and it is easy to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate logistic regression for modelling. For the lbfgs solvers set verbose to any positive \n",
    "#   number for verbosity.\n",
    "logit = LogisticRegression(n_jobs=4, solver='lbfgs', verbose=1, max_iter=1000000)\n",
    "\n",
    "\n",
    "# Apply TF-IDF to our train and test text fields\n",
    "X_train_transformed = tf_idf.fit_transform(X_train)\n",
    "X_test_transformed = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Train our model and predict over the test dataset\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Train the train dataset with Logistic Regression\n",
    "logit.fit(X_train_transformed, Y_train)\n",
    "\n",
    "# Predict labels (is the comment sarcastic or not) on the test dataset\n",
    "predictions_values = logit.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the results of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Evaluate the results\n",
    "###\n",
    "################################################################################################### \n",
    "        \n",
    "\n",
    "# Inidicate when it finishes\n",
    "end_time = time.time()\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy_score(Y_test, predictions_values)\n",
    "print(\"The logistic regression model predicts correctly %.2f percent of the Reddit comments\"%(accuracy_score(Y_test, predictions_values)*100))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(Y_test, predictions_values, ['genuine','sarcastic'], figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results on the classification report\n",
    "print(classification_report(Y_test, predictions_values))\n",
    "\n",
    "print(\"Our model predicts slightly worse the sarcastic comments (0.72), but there is not a large deviation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In general, an AUC of 0.5 suggests no discrimination (i.e., ability to classify a comment \n",
    "#  with and without knowing the condition), 0.7 to 0.8 is considered acceptable, 0.8 to 0.9 \n",
    "#  is considered excellent and more than 0.9 is considered outstanding\")\n",
    "\n",
    "print(\"Our model provided an AUC score of\", round(roc_auc_score(Y_test, logit.predict(X_test_transformed)),2), \", meaning, that the model is not optimal, but on an acceptable range\")\n",
    "print(\"The more far left the curve is, the better our model. We can adjust our threshold based on our ROC curve to get results based on model requirements\") \n",
    "    \n",
    "# Set up visualization for the ROC curve\n",
    "logit_roc_auc = roc_auc_score(Y_test, logit.predict(X_test_transformed))\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, logit.predict_proba(X_test_transformed)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Store the results\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Store the final results from the logistic regression model without cleaning (since it was our best performer)\n",
    "logit_results = pd.DataFrame({'id': test_df_model.id, 'predicted': predictions_values})\n",
    "logit_results.to_csv(\"log_reg_results.csv\")\n",
    "\n",
    "# Store the results for comparison\n",
    "score = round(accuracy_score(Y_test, predictions_values),2)\n",
    "model_name = 'log_reg_comment_only_without_cleaning'\n",
    "\n",
    "# Create a comparison table to append the results\n",
    "comparison_table = pd.DataFrame([[model_name, score, round(end_time - start_time,0)]], columns = ['Model', 'Accuracy', 'Execution_Time_Seconds'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Save our trained model to file and load it later in order to make predictions\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Save the model to disk\n",
    "model_saved = 'logistic_regression_model_sarcasm.sav'\n",
    "pickle.dump(logit, open(model_saved, 'wb'))\n",
    " \n",
    "# Load the model from disk\n",
    "model_saved = 'logistic_regression_model_sarcasm.sav'\n",
    "loaded_log_regression = pickle.load(open(model_saved, 'rb'))\n",
    "\n",
    "# Show accuracy score from our model\n",
    "print(\"Our model accuracy is \", round(loaded_log_regression.score(X_test_transformed, Y_test),2))\n",
    "\n",
    "# Show predictions from loaded our model\n",
    "print(\"Some results of our prediction (labels): \" ,loaded_log_regression.predict(X_test_transformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Logistic Regression on 'comment' + 'parent_comment' fields without data cleaning\n",
    "\n",
    "In this case, we will use 'comment' and 'parent_comment' as the text input field for the model, in order to understand if this approach provides higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =      1768994     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.30554D+05    |proj g|=  7.73580D+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  4.95498D+05    |proj g|=  3.72044D+02\n",
      "\n",
      "At iterate  100    f=  4.91503D+05    |proj g|=  7.51449D+01\n",
      "\n",
      "At iterate  150    f=  4.91306D+05    |proj g|=  4.63508D+02\n",
      "\n",
      "At iterate  200    f=  4.91231D+05    |proj g|=  1.90695D+02\n",
      "\n",
      "At iterate  250    f=  4.91201D+05    |proj g|=  1.88876D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "*****    264    313      1     0     0   5.462D+00   4.912D+05\n",
      "  F =   491200.78147815133     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "The logistic regression model predicts correctly 69.11 percent of the Reddit comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Prepare the datasets for the model\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Initiate\n",
    "start_time = time.time()\n",
    "\n",
    "# Set datasets\n",
    "train_df_model = train_df\n",
    "test_df_model = test_df\n",
    "\n",
    "# Create column with comment and parent comment text together. This will become the field to model over\n",
    "train_df_model['comment_and_parent_comment'] = train_df_model['comment'] + ' ' + train_df_model['parent_comment'] \n",
    "train_df_model = train_df_model[['id', 'label', 'comment_and_parent_comment']].dropna()\n",
    "\n",
    "# Create column with comment and parent comment text together. This will become the field to model over\n",
    "test_df_model['comment_and_parent_comment'] = test_df_model['comment'] + ' ' + test_df_model['parent_comment'] \n",
    "test_df_modelt_df = test_df_model[['id', 'label', 'comment_and_parent_comment']].dropna()\n",
    "\n",
    "# Keep only the necessary columns\n",
    "test_df_model = test_df[['id', 'label', 'comment_and_parent_comment']].dropna()\n",
    "train_df_model = train_df[['id', 'label', 'comment_and_parent_comment']].dropna()\n",
    "\n",
    "# Set training dataset\n",
    "X_train = train_df_model['comment_and_parent_comment']\n",
    "Y_train = train_df_model['label']\n",
    "\n",
    "# # Set test dataset\n",
    "X_test = test_df_model['comment_and_parent_comment']\n",
    "Y_test = test_df_model['label']\n",
    "\n",
    "# Show how does the dataframe look like\n",
    "train_df_model.head()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###\n",
    "### Train our model and predict over the test dataset\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Initiate logistic regression for modelling\n",
    "logit = LogisticRegression(n_jobs=4, solver='lbfgs', verbose=1, max_iter=1000000)\n",
    "\n",
    "# Apply TF-IDF to our train and test text fields\n",
    "X_train_transformed = tf_idf.fit_transform(X_train)\n",
    "X_test_transformed = tf_idf.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "logit.fit(X_train_transformed, Y_train)\n",
    "\n",
    "# Predict labels (is the comment sarcastic or not) on the test dataset\n",
    "predictions_values = logit.predict(X_test_transformed)\n",
    "\n",
    "# Inidicate when it finishes\n",
    "end_time = time.time()\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy_score(Y_test, predictions_values)\n",
    "print(\"The logistic regression model predicts correctly %.2f percent of the Reddit comments\"%(accuracy_score(Y_test, predictions_values)*100))\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###\n",
    "### Store the results\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Store the results for comparison\n",
    "score = round(accuracy_score(Y_test, predictions_values),2)\n",
    "model_name = 'log_reg_comment_with_parent_without_cleaning'\n",
    "df_new_comparison = pd.DataFrame([[model_name, score, round(end_time - start_time,0)]], columns = ['Model', 'Accuracy', 'Execution_Time_Seconds'])\n",
    "\n",
    "# Append the results to the final comparison table\n",
    "comparison_table = comparison_table.append(df_new_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Logistic Regression on 'comment' field with data cleaning\n",
    "\n",
    "In this case, we will use 'comment' as the text input field for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =       476361     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.30554D+05    |proj g|=  2.41215D+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  4.96390D+05    |proj g|=  7.83736D+01\n",
      "\n",
      "At iterate  100    f=  4.95253D+05    |proj g|=  2.31906D+01\n",
      "\n",
      "At iterate  150    f=  4.95232D+05    |proj g|=  5.25829D+00\n",
      "\n",
      "At iterate  200    f=  4.95230D+05    |proj g|=  9.61505D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "*****    219    261      1     0     0   1.655D+01   4.952D+05\n",
      "  F =   495229.24655982602     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "The logistic regression model predicts correctly 69.80 percent of the Reddit comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   31.1s finished\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Prepare the datasets for the model\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Initiate\n",
    "start_time = time.time()\n",
    "\n",
    "# Keep only the necessary columns\n",
    "test_df_model = test_df[['id', 'label', 'comment']].dropna()\n",
    "train_df_model = train_df[['id', 'label', 'comment']].dropna()\n",
    "\n",
    "# Set training dataset\n",
    "X_train = train_df_model['comment']\n",
    "Y_train = train_df_model['label']\n",
    "\n",
    "# Set test dataset\n",
    "X_test = test_df_model['comment']\n",
    "Y_test = test_df_model['label']\n",
    "\n",
    "# Apply cleaning functions (functions.py) to our text data\n",
    "test_df_model['comment'] =  test_df_model['comment'].apply(replace_contractions).apply(basic_cleaning).apply(remove_stopwords)\n",
    "train_df_model['comment'] =  train_df_model['comment'].apply(replace_contractions).apply(basic_cleaning).apply(remove_stopwords)\n",
    "\n",
    "# Show how does the dataframe look like\n",
    "train_df_model.head()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###\n",
    "### Train our model and predict over the test dataset\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Initiate logistic regression for modelling\n",
    "logit = LogisticRegression(n_jobs=4, solver='lbfgs', verbose=1, max_iter=1000000)\n",
    "\n",
    "# Apply TF-IDF to our train and test text fields\n",
    "X_train_transformed = tf_idf.fit_transform(X_train)\n",
    "X_test_transformed = tf_idf.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "logit.fit(X_train_transformed, Y_train)\n",
    "\n",
    "# Predict labels (is the comment sarcastic or not) on the test dataset\n",
    "predictions_values = logit.predict(X_test_transformed)\n",
    "\n",
    "# Inidicate when it finishes\n",
    "end_time = time.time()\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy_score(Y_test, predictions_values)\n",
    "print(\"The logistic regression model predicts correctly %.2f percent of the Reddit comments\"%(accuracy_score(Y_test, predictions_values)*100))\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###\n",
    "### Store the results\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Store the results for comparison\n",
    "score = round(accuracy_score(Y_test, predictions_values),2)\n",
    "model_name = 'log_reg_comment_only_with_cleaning'\n",
    "df_new_comparison = pd.DataFrame([[model_name, score, round(end_time - start_time,0)]], columns = ['Model', 'Accuracy', 'Execution_Time_Seconds'])\n",
    "\n",
    "# Append the results to the final comparison table\n",
    "comparison_table = comparison_table.append(df_new_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Logistic Regression on 'comment' + 'parent_comment' fields with data cleaning\n",
    "\n",
    "In this case, we will use 'comment' and 'parent_comment' as the text input field for the model, in order to understand if this approach provides higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =      1745429     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.30554D+05    |proj g|=  9.17238D+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  5.06450D+05    |proj g|=  1.67644D+02\n",
      "\n",
      "At iterate  100    f=  5.04682D+05    |proj g|=  8.98742D+00\n",
      "\n",
      "At iterate  150    f=  5.04634D+05    |proj g|=  9.75542D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "*****    184    226      1     0     0   8.531D+00   5.046D+05\n",
      "  F =   504632.21591279254     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "The logistic regression model predicts correctly 67.01 percent of the Reddit comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Prepare the datasets for the model\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Initiate\n",
    "start_time = time.time()\n",
    "\n",
    "# Set datasets\n",
    "train_df_model = train_df\n",
    "test_df_model = test_df\n",
    "\n",
    "# Create column with comment and parent comment text together. This will become the field to model over\n",
    "train_df_model['comment_and_parent_comment'] = train_df_model['comment'] + ' ' + train_df_model['parent_comment'] \n",
    "train_df_model = train_df_model[['id', 'label', 'comment_and_parent_comment']].dropna()\n",
    "\n",
    "# Create column with comment and parent comment text together. This will become the field to model over\n",
    "test_df_model['comment_and_parent_comment'] = test_df_model['comment'] + ' ' + test_df_model['parent_comment'] \n",
    "test_df_modelt_df = test_df_model[['id', 'label', 'comment_and_parent_comment']].dropna()\n",
    "\n",
    "# Keep only the necessary columns\n",
    "test_df_model = test_df[['id', 'label', 'comment_and_parent_comment']].dropna()\n",
    "train_df_model = train_df[['id', 'label', 'comment_and_parent_comment']].dropna()\n",
    "\n",
    "# Apply cleaning functions (functions.py) to our text data\n",
    "test_df_model['comment_and_parent_comment'] =  test_df_model['comment_and_parent_comment'].apply(replace_contractions).apply(basic_cleaning).apply(remove_stopwords)\n",
    "train_df_model['comment_and_parent_comment'] =  train_df_model['comment_and_parent_comment'].apply(replace_contractions).apply(basic_cleaning).apply(remove_stopwords)\n",
    "\n",
    "# Set training dataset\n",
    "X_train = train_df_model['comment_and_parent_comment']\n",
    "Y_train = train_df_model['label']\n",
    "\n",
    "# # Set test dataset\n",
    "X_test = test_df_model['comment_and_parent_comment']\n",
    "Y_test = test_df_model['label']\n",
    "\n",
    "# Show how does the dataframe look like\n",
    "train_df_model.head()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###\n",
    "### Train our model and predict over the test dataset\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Initiate logistic regression for modelling\n",
    "logit = LogisticRegression(n_jobs=4, solver='lbfgs', verbose=1, max_iter=1000000)\n",
    "\n",
    "# Apply TF-IDF to our train and test text fields\n",
    "X_train_transformed = tf_idf.fit_transform(X_train)\n",
    "X_test_transformed = tf_idf.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "logit.fit(X_train_transformed, Y_train)\n",
    "\n",
    "# Predict labels (is the comment sarcastic or not) on the test dataset\n",
    "predictions_values = logit.predict(X_test_transformed)\n",
    "\n",
    "# Inidicate when it finishes\n",
    "end_time = time.time()\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy_score(Y_test, predictions_values)\n",
    "print(\"The logistic regression model predicts correctly %.2f percent of the Reddit comments\"%(accuracy_score(Y_test, predictions_values)*100))\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###\n",
    "### Store the results\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "# Store the results for comparison\n",
    "score = round(accuracy_score(Y_test, predictions_values),2)\n",
    "model_name = 'log_reg_comment_with_parent_with_cleaning'\n",
    "df_new_comparison = pd.DataFrame([[model_name, score, round(end_time - start_time,0)]], columns = ['Model', 'Accuracy', 'Execution_Time_Seconds'])\n",
    "\n",
    "# Append the results to the final comparison table\n",
    "comparison_table = comparison_table.append(df_new_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Model  Accuracy  \\\n",
      "0         log_reg_comment_only_without_cleaning      0.73   \n",
      "0  log_reg_comment_with_parent_without_cleaning      0.69   \n",
      "0            log_reg_comment_only_with_cleaning      0.70   \n",
      "0     log_reg_comment_with_parent_with_cleaning      0.67   \n",
      "\n",
      "   Execution_Time_Seconds  \n",
      "0                    74.0  \n",
      "0                   182.0  \n",
      "0                   893.0  \n",
      "0                   978.0  \n"
     ]
    }
   ],
   "source": [
    "###################################################################################################\n",
    "###\n",
    "### Save the final results\n",
    "###\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(comparison_table)\n",
    "\n",
    "# Save the results to csv - we will compare later on with the rest of the models' outputs\n",
    "comparison_table.to_csv(\"comparison_table_log_reg.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
